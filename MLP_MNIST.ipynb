{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXSVCIhz_bJ2"
      },
      "source": [
        "# Лабораторная работа №1\n",
        "\n",
        "**Многослойный перцептрон**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz2fizZEEE8J"
      },
      "source": [
        "В PyTorch изучаемые параметры (т. е. веса и смещения) модели содержатся в параметрах torch.nn.Module модели (доступ к которым осуществляется с помощью ).\n",
        "State_dict — это просто объект словаря Python, который сопоставляет каждый слой со своим тензором параметров .\n",
        "Только слои с обучаемыми параметрами (сверточные слои, линейные слои и т. д.) и зарегистрированными буферами (running_mean пакета Batchnorm) имеют записи в state_dict модели . Объекты оптимизатора ( ) также имеют state_dict , который содержит информацию о состоянии оптимизатора, а также об используемых гиперпараметрах.model.parameters()torch.optim\n",
        "\n",
        "Поскольку объекты state_dict представляют собой словари Python, их можно легко сохранять, обновлять, изменять и восстанавливать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hjbtVKyE3fB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.optim import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Skv2ksEW0a",
        "outputId": "0ea1a52b-988c-4879-96e6-5066f06b89c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вывод словаря модели:\n",
            "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
            "conv1.bias \t torch.Size([6])\n",
            "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
            "conv2.bias \t torch.Size([16])\n",
            "fc1.weight \t torch.Size([120, 400])\n",
            "fc1.bias \t torch.Size([120])\n",
            "fc2.weight \t torch.Size([84, 120])\n",
            "fc2.bias \t torch.Size([84])\n",
            "fc3.weight \t torch.Size([10, 84])\n",
            "fc3.bias \t torch.Size([10])\n",
            "Optimizer's state_dict:\n",
            "state \t {}\n",
            "param_groups \t [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
          ]
        }
      ],
      "source": [
        "class TheModelClass(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TheModelClass, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Инициализация модели\n",
        "model = TheModelClass()\n",
        "\n",
        "# инициализация оптимизатора\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Вывод словаря модели\n",
        "print(\"Вывод словаря модели:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Вывод словаря оптимизатора\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBidNdYlF2cu"
      },
      "source": [
        "Сохранение и загрузка модели для вывода\n",
        "\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKUCdApxKqpV"
      },
      "source": [
        "## Чтение данных и dataset\n",
        "\n",
        "Из встроенных датасетов torchvision загрузите тестовую и обучающую выборки из MNIST, указав преобразование для изображений используя ToTensor()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBl3CId8AQfm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Настройки для графиков\n",
        "%matplotlib inline\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_p302aCAeCd"
      },
      "source": [
        "## Загрузка данных\n",
        "\n",
        "Из встроенных датасетов torchvision загрузите тестовую и обучающую выборки из MNIST, указав приобразование для изображений используя ToTensor().\n",
        "\n",
        "https://pytorch.org/vision/main/generated/torchvision.datasets.EMNIST.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXIylPhVKy22"
      },
      "outputs": [],
      "source": [
        "train_set = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4xVF9NlVnTA"
      },
      "outputs": [],
      "source": [
        "test_set = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFx_fOFRVnTA",
        "outputId": "5df92c67-603f-4366-fe1d-d831f831a9d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXrw-DolX5I4"
      },
      "source": [
        "Выведите информацию о количестве данных в обеих выборках, размерностях изображений, количестве классов и сами метки классов.\n",
        "\n",
        "Функция dir() в python возвращает список допустимых атрибутов объекта, что может вам подсказать как получить часть информации о датасете.\n",
        "Датасеты позволяют получить общее количество объектов с помощью функции `len`, также - объект с классом по индексу. В атрибуте `classes` хранятся ярлыки классов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmtxehqAVnTA",
        "outputId": "49f7ecbf-01d4-46dc-8790-c1a70ffa5225"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_set.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMDp5mMiVnTB",
        "outputId": "f0707c7d-4062-46ce-d697-6e99a7be6724"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__orig_bases__',\n",
              " '__parameters__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_check_exists',\n",
              " '_check_legacy_exist',\n",
              " '_format_transform_repr',\n",
              " '_is_protocol',\n",
              " '_load_data',\n",
              " '_load_legacy_data',\n",
              " '_repr_indent',\n",
              " 'class_to_idx',\n",
              " 'classes',\n",
              " 'data',\n",
              " 'download',\n",
              " 'extra_repr',\n",
              " 'mirrors',\n",
              " 'processed_folder',\n",
              " 'raw_folder',\n",
              " 'resources',\n",
              " 'root',\n",
              " 'target_transform',\n",
              " 'targets',\n",
              " 'test_data',\n",
              " 'test_file',\n",
              " 'test_labels',\n",
              " 'train',\n",
              " 'train_data',\n",
              " 'train_labels',\n",
              " 'training_file',\n",
              " 'transform',\n",
              " 'transforms']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dir(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPyS_M2MQIkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074d59bf-4da5-4001-8653-4381587de975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 10000 torch.Size([60000, 28, 28]) 10 ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
          ]
        }
      ],
      "source": [
        "train_samples_len = len(train_set)\n",
        "test_samples_len = len(test_set)\n",
        "image_shape = train_set.data.shape\n",
        "classes_len = len(train_set.classes)\n",
        "classes_labels = train_set.classes\n",
        "\n",
        "print(train_samples_len, test_samples_len, image_shape, classes_len, classes_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp22upbILX-G"
      },
      "source": [
        "Вывод 9 экземпляров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtlfw1fuLb56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "34e3a8ff-5645-4d63-fcb8-f57b187b4d27"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "labels_map = dict(zip(range(len(train_set.classes)), train_set.classes))\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_set), size=(1,)).item()\n",
        "    img, label = train_set[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yoIxKt9KzNA"
      },
      "source": [
        "## Создание dataloader\n",
        "\n",
        "Данные при обучении модели редко передаются по одному образцу или все разом, обычно образцы объединяются в пакеты (batches) и уже они передаются на вход модели.\n",
        "\n",
        "Размер пакета (batch size) выбирается как $2^n$ (16, 32, 64, 128) .\n",
        "\n",
        "Загрузчики данных будут использоваться при обучении и тестировании модели. Чтобы между эпохами модель на обучалась на одинаковых пакетах их перемешивают (shuffle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkWHD_HlK2oX"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b33-Y41eQtEW"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, 32, shuffle=True)\n",
        "test_loader = DataLoader(test_set, 32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arv3rg8iK4q1"
      },
      "source": [
        "## Создание модели\n",
        "В этой работе будеут использованы контейнеры pytorch для построения нейронной сети, например `nn.Sequential` или `nn.Module`.\n",
        "\n",
        "В этой работе, используя функции из модуля `torch.nn.functional` `linear` и `relu`, необходимо собрать цепочку вычислений для получения выходных значений.\n",
        "При инициализации MLP необходимо создать несколько линейных слоев и использовать функцию активации (например ReLU), которые будут использоваться при прямом проходе в модели. Перед входным слоем необходимо использовать для изображений `.flatten()'.\n",
        "\n",
        "- Линейный слой `nn.Linear`.\n",
        "- Функция активации `nn.ReLU`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCsoBNmjK3xP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mZ6znoKRGCF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ComplexMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "        self.bn6 = nn.BatchNorm1d(128)\n",
        "        self.bn7 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn6(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn7(self.fc4(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUkYNQM8GF6_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AthXSoi2Rp-L"
      },
      "outputs": [],
      "source": [
        "model = ComplexMLP()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng4GlzQufHLo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2477bf20-17e5-4c9c-8362-b1e368a3ab8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ComplexMLP(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h66fFz8ue1Nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0c6a6c-c385-42a8-d5fd-9b7eb342bdce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 3, 3])\n",
            "torch.Size([32])\n",
            "torch.Size([64, 32, 3, 3])\n",
            "torch.Size([64])\n",
            "torch.Size([128, 64, 3, 3])\n",
            "torch.Size([128])\n",
            "torch.Size([512, 1152])\n",
            "torch.Size([512])\n",
            "torch.Size([256, 512])\n",
            "torch.Size([256])\n",
            "torch.Size([128, 256])\n",
            "torch.Size([128])\n",
            "torch.Size([64, 128])\n",
            "torch.Size([64])\n",
            "torch.Size([10, 64])\n",
            "torch.Size([10])\n",
            "torch.Size([32])\n",
            "torch.Size([32])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([512])\n",
            "torch.Size([512])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "torch.Size([64])\n",
            "torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "for param in model.parameters(): # вывод параметров модели\n",
        "  print(param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rleu44UqYYxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b423293c-02a9-41af-dda9-d094a113fa37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight \t torch.Size([32, 1, 3, 3])\n",
            "conv1.bias \t torch.Size([32])\n",
            "conv2.weight \t torch.Size([64, 32, 3, 3])\n",
            "conv2.bias \t torch.Size([64])\n",
            "conv3.weight \t torch.Size([128, 64, 3, 3])\n",
            "conv3.bias \t torch.Size([128])\n",
            "fc1.weight \t torch.Size([512, 1152])\n",
            "fc1.bias \t torch.Size([512])\n",
            "fc2.weight \t torch.Size([256, 512])\n",
            "fc2.bias \t torch.Size([256])\n",
            "fc3.weight \t torch.Size([128, 256])\n",
            "fc3.bias \t torch.Size([128])\n",
            "fc4.weight \t torch.Size([64, 128])\n",
            "fc4.bias \t torch.Size([64])\n",
            "fc5.weight \t torch.Size([10, 64])\n",
            "fc5.bias \t torch.Size([10])\n",
            "bn1.weight \t torch.Size([32])\n",
            "bn1.bias \t torch.Size([32])\n",
            "bn1.running_mean \t torch.Size([32])\n",
            "bn1.running_var \t torch.Size([32])\n",
            "bn1.num_batches_tracked \t torch.Size([])\n",
            "bn2.weight \t torch.Size([64])\n",
            "bn2.bias \t torch.Size([64])\n",
            "bn2.running_mean \t torch.Size([64])\n",
            "bn2.running_var \t torch.Size([64])\n",
            "bn2.num_batches_tracked \t torch.Size([])\n",
            "bn3.weight \t torch.Size([128])\n",
            "bn3.bias \t torch.Size([128])\n",
            "bn3.running_mean \t torch.Size([128])\n",
            "bn3.running_var \t torch.Size([128])\n",
            "bn3.num_batches_tracked \t torch.Size([])\n",
            "bn4.weight \t torch.Size([512])\n",
            "bn4.bias \t torch.Size([512])\n",
            "bn4.running_mean \t torch.Size([512])\n",
            "bn4.running_var \t torch.Size([512])\n",
            "bn4.num_batches_tracked \t torch.Size([])\n",
            "bn5.weight \t torch.Size([256])\n",
            "bn5.bias \t torch.Size([256])\n",
            "bn5.running_mean \t torch.Size([256])\n",
            "bn5.running_var \t torch.Size([256])\n",
            "bn5.num_batches_tracked \t torch.Size([])\n",
            "bn6.weight \t torch.Size([128])\n",
            "bn6.bias \t torch.Size([128])\n",
            "bn6.running_mean \t torch.Size([128])\n",
            "bn6.running_var \t torch.Size([128])\n",
            "bn6.num_batches_tracked \t torch.Size([])\n",
            "bn7.weight \t torch.Size([64])\n",
            "bn7.bias \t torch.Size([64])\n",
            "bn7.running_mean \t torch.Size([64])\n",
            "bn7.running_var \t torch.Size([64])\n",
            "bn7.num_batches_tracked \t torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvZyl5GJYbgs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKLVRCz9e6MQ"
      },
      "source": [
        "**Сколько параметров в вашей модели?**\n",
        "\n",
        "Ваш ответ: 857506"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GtkUPz_K7OO"
      },
      "source": [
        "## Функция потерь и оптимизатор\n",
        "\n",
        "В задаче классификации чаще всего используется функция потерь на основе перекрестной энтропии.\n",
        "\n",
        "В качестве оптимизатора для параметров модели можно выбрать стохастический градиентный спуск или Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49_C7QmhTE4a"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luq7PGDSK_yM"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD\n",
        "\n",
        "LR = 0.01\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScI4FVl2INE7"
      },
      "source": [
        "## Цикл обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNmm7GGbISPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add1b3d4-5ec5-433e-8ba6-0539ab8bca34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха №1\n",
            "Потери на обучающей выборке 0.78957\n",
            "Точность на обучающей выборке: 78.45%\n",
            "Потери на тестовой выборке 0.07104\n",
            "Точность на тестовой выборке: 98.37%\n",
            "Эпоха №2\n",
            "Потери на обучающей выборке 0.24977\n",
            "Точность на обучающей выборке: 94.37%\n",
            "Потери на тестовой выборке 0.04482\n",
            "Точность на тестовой выборке: 98.81%\n",
            "Эпоха №3\n",
            "Потери на обучающей выборке 0.18279\n",
            "Точность на обучающей выборке: 95.90%\n",
            "Потери на тестовой выборке 0.03165\n",
            "Точность на тестовой выборке: 99.07%\n",
            "Эпоха №4\n",
            "Потери на обучающей выборке 0.14894\n",
            "Точность на обучающей выборке: 96.62%\n",
            "Потери на тестовой выборке 0.03132\n",
            "Точность на тестовой выборке: 99.05%\n",
            "Эпоха №5\n",
            "Потери на обучающей выборке 0.12744\n",
            "Точность на обучающей выборке: 97.16%\n",
            "Потери на тестовой выборке 0.03365\n",
            "Точность на тестовой выборке: 99.10%\n",
            "Эпоха №6\n",
            "Потери на обучающей выборке 0.11151\n",
            "Точность на обучающей выборке: 97.47%\n",
            "Потери на тестовой выборке 0.02714\n",
            "Точность на тестовой выборке: 99.24%\n",
            "Эпоха №7\n",
            "Потери на обучающей выборке 0.09874\n",
            "Точность на обучающей выборке: 97.85%\n",
            "Потери на тестовой выборке 0.02807\n",
            "Точность на тестовой выборке: 99.28%\n",
            "Эпоха №8\n",
            "Потери на обучающей выборке 0.09878\n",
            "Точность на обучающей выборке: 97.81%\n",
            "Потери на тестовой выборке 0.02588\n",
            "Точность на тестовой выборке: 99.34%\n",
            "Эпоха №9\n",
            "Потери на обучающей выборке 0.08502\n",
            "Точность на обучающей выборке: 98.13%\n",
            "Потери на тестовой выборке 0.02406\n",
            "Точность на тестовой выборке: 99.34%\n",
            "Эпоха №10\n",
            "Потери на обучающей выборке 0.08202\n",
            "Точность на обучающей выборке: 98.29%\n",
            "Потери на тестовой выборке 0.02822\n",
            "Точность на тестовой выборке: 99.27%\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  train_loss = 0\n",
        "  train_correct = 0\n",
        "  test_loss = 0\n",
        "  test_correct = 0\n",
        "  print(f\"Эпоха №{epoch+1}\")\n",
        "  model.train() # переключение модели в режим обучения\n",
        "  for imgs, labels in train_loader:\n",
        "    #\n",
        "    # Классический цикл обучения\n",
        "    # 1. Обнуление градиентов с помощью метода zero_grad у оптимизатора\n",
        "    # 2. Получение вывода модели по пакету данных\n",
        "    # 3. Вычисление потерь на основе вывода модели и исходных значений\n",
        "    # 4. Вычисление градиентов для параметров\n",
        "    # 5. Обновление параметров с помощью шага \".step()\" у оптимизатора\n",
        "    # 6. Вычисление метрики\n",
        "        optimizer.zero_grad() # 1. Обнуление градиентов с помощью метода zero_grad у оптимизатора\n",
        "        outputs = model(imgs)  # 2. Получение вывода модели по пакету данных\n",
        "          # outputs = model(imgs.flatten(start_dim=1))\n",
        "        loss = loss_fn(outputs, labels)  # 3. Вычисление потерь на основе вывода модели и исходных значений\n",
        "        loss.backward()  # 4. Вычисление градиентов для параметров\n",
        "        optimizer.step()  # 5. Обновление параметров с помощью шага \".step()\" у оптимизатора\n",
        "    # 6. Вычисление метрики\n",
        "\n",
        "        train_loss += loss.item() # прибавьте потери с этой итерации\n",
        "    # метод .item() - возвращает значение этого тензора\n",
        "    # в виде стандартного числа Python.\n",
        "        train_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "  # train_correct += torch.sum(torch.argmax(outputs, dim=1) == labels).item() # Необходимо получить сумму совпавщих значений между исходными индексами классов (labels)\n",
        "    # и индексами максимальных значений в пакетах данных.\n",
        "    # Вам пригодятся torch.sum, torch.argmax и .item()\n",
        "\n",
        "  avg_train_loss = train_loss / len(train_loader) # полученную сумму потерь необходимо разделить на количество пакетов данных len(train_loader)\n",
        "  train_acc = train_correct / len(train_set) # полученное количество правильно классифицированных изображений\n",
        "  # необходимо разделить на количество изображений в обучающей выборке\n",
        "  print(f\"Потери на обучающей выборке {avg_train_loss:.5f}\")\n",
        "  print(f\"Точность на обучающей выборке: {train_acc*100:.2f}%\")\n",
        "\n",
        "\n",
        "  model.eval() # переключение модели в режим оценивания\n",
        "  for imgs, labels in test_loader:\n",
        "    with torch.no_grad(): # работа в контексте отключенного вычисления градиентов\n",
        "      # Для оценки модели:\n",
        "      outputs = model(imgs) # 1. Получение вывода модели по пакету данных\n",
        "      loss = loss_fn(outputs, labels)# 2. Вычисление потерь на основе вывода модели и исходных значений\n",
        "      # 3. Вычисление метрики\n",
        "\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      test_correct += torch.sum(torch.argmax(outputs, dim=1) == labels).item()\n",
        "\n",
        "  avg_test_loss = test_loss / len(test_loader)\n",
        "  test_acc =  test_correct / len(test_set)\n",
        "  print(f\"Потери на тестовой выборке {avg_test_loss:.5f}\")\n",
        "  print(f\"Точность на тестовой выборке: {test_acc*100:.2f}%\")\n",
        "\n",
        "  if test_acc > best_acc:\n",
        "    best_acc = test_acc\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            }, \"best_model_params.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIpCgKmSq2kl"
      },
      "source": [
        "Для оценки модели нужно отключать вычисление градиентов `with torch.no_grad()`?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHpTqJ_yIJAo"
      },
      "source": [
        "## Загрузка модели и инференс\n",
        "\n",
        "Проверка модели на цифрах.\n",
        "\n",
        "Необходимо нарисовать цифру в любом графическом редакторе (Paint, Gimp, Photoshop).\n",
        "\n",
        "Изображение должно быть черно-белое, ширина и высота 28 px, черный фон, белая цифра. Формат  png, jpg.\n",
        "\n",
        "Для загрузки изображения в Google Colab в боковой панели откройте Files (1), и выберите вариант Upload to session storage (2).\n",
        "\n",
        "Затем файл необходимо считать и подготовить перед тем, как передать в модель.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEvcMIyFCOUn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "outputId": "4b3bbed5-e71e-4bf9-aeb6-353099fef9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ширина и высота (28, 28), количество каналов 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAQUlEQVR4AWNgGLKAmRXV6UzIXDlRZB4DA7KkAPM3VElGJC4bG8sHJC6qzl//UKRQJRkY/qPKsiBz/yBzRtlDMQQAjvsGRdjWHAAAAAAASUVORK5CYII=\n",
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+lCkqWAOB1PpSUUU8ORGyc4OOKZRRRRRRRRRX/9k=\n"
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "from PIL import Image\n",
        "img = Image.open('1_negate (1).png')\n",
        "print(f\"Ширина и высота {img.size}, количество каналов {len(img.mode)}\")\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywx9wLJrLrVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ce4b9d-eab5-4503-9421-9aefcfae4101"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "# Раскомментируйте, если цветное изображение\n",
        "# transform_grayscale = transforms.Grayscale()\n",
        "# img = transform_grayscale(img)\n",
        "\n",
        "# Преобразования изображения\n",
        "transform_to_tensor = transforms.ToTensor()\n",
        "x = transform_to_tensor(img)\n",
        "\n",
        "# Выведите размерности тензора x\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIFcW_g2ND1g"
      },
      "source": [
        "Модель строилась с использованием пакетов данных, которые создавали экземпляры класса DataLoader, для использования одного изображения нужно создать пакет из одного изображения с использованием  метода тензора `unsqueeze` или `unsqueeze_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hru4DrGZMV8Q"
      },
      "outputs": [],
      "source": [
        "x = torch.unsqueeze(x, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZP1H5VwOMO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad588f1-304b-45db-b5cc-a1c75f3ce54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-dbc638b35d56>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model_params = torch.load(\"best_model_params.pth\") # Укажите путь до сохраненного файла\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ComplexMLP(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1152, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "best_model = ComplexMLP()# Новый экземпляр класса модели\n",
        "best_model_params = torch.load(\"best_model_params.pth\") # Укажите путь до сохраненного файла\n",
        "state_dict = best_model_params['model_state_dict']# Из best_model_params извлеките по соответствующему ключу параметры модели\n",
        "# Для best_model используйте метод load_state_dict и передайте в него словарь с параметрами.\n",
        "# Переключите модель в режим проверки\n",
        "\n",
        "best_model.load_state_dict(state_dict)\n",
        "best_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN2s_oNuQDzQ"
      },
      "source": [
        "Тестирование модели\n",
        " Передайте на вход модели подготовленный тензор. Из вывода модели получите индекс наибольшего значения (`argmax`) и по индексу получите результат из массива ярлыков классов (`test_set.classes`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZbLYZvDRd5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9062233-bf6b-4911-8421-d3716a34de38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанный класс: 1 - one\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    output = best_model(x)\n",
        "\n",
        "predicted_index = torch.argmax(output, dim=1).item()\n",
        "predicted_class = test_set.classes[predicted_index]\n",
        "\n",
        "print(f\"Предсказанный класс: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A0E5_D4RgFU"
      },
      "source": [
        "**Правильно ли предсказала модель цифру?**\n",
        "\n",
        "Ваш ответ: Да\n",
        "\n",
        "**Лучшая точность на тестовой выборке для вашей модели.**\n",
        "\n",
        "Ваш ответ: 99.34%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1oOEgCxUDsp"
      },
      "source": [
        "\n",
        "## Задание 1.\n",
        "\n",
        "1. Подбор гиперпараметров\n",
        "Используя рабочий код модели и ее обучения, используйте разные варианты скорости обучения, количества параметров в скрытых слоя модели, количества эпох, размера батчей.\n",
        "2. Выведите гиперпараметры лучшей  модели.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLghoYupUn2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33157a3-4eea-4e52-8bc7-d8e1db5449d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: lr=0.001, hidden_size=64, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.9515\n",
            "Testing: lr=0.001, hidden_size=64, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.9437\n",
            "Testing: lr=0.001, hidden_size=64, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.9370\n",
            "Testing: lr=0.001, hidden_size=128, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.9597\n",
            "Testing: lr=0.001, hidden_size=128, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.9602\n",
            "Testing: lr=0.001, hidden_size=128, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.9537\n",
            "Testing: lr=0.001, hidden_size=256, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.9736\n",
            "Testing: lr=0.001, hidden_size=256, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.9653\n",
            "Testing: lr=0.001, hidden_size=256, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.9650\n",
            "Testing: lr=0.01, hidden_size=64, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.9437\n",
            "Testing: lr=0.01, hidden_size=64, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.9493\n",
            "Testing: lr=0.01, hidden_size=64, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.9530\n",
            "Testing: lr=0.01, hidden_size=128, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.9547\n",
            "Testing: lr=0.01, hidden_size=128, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.9520\n",
            "Testing: lr=0.01, hidden_size=128, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.9567\n",
            "Testing: lr=0.01, hidden_size=256, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.9381\n",
            "Testing: lr=0.01, hidden_size=256, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.9517\n",
            "Testing: lr=0.01, hidden_size=256, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.9620\n",
            "Testing: lr=0.1, hidden_size=64, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.3008\n",
            "Testing: lr=0.1, hidden_size=64, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.3022\n",
            "Testing: lr=0.1, hidden_size=64, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.8034\n",
            "Testing: lr=0.1, hidden_size=128, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.1122\n",
            "Testing: lr=0.1, hidden_size=128, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.3517\n",
            "Testing: lr=0.1, hidden_size=128, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.3572\n",
            "Testing: lr=0.1, hidden_size=256, batch_size=32, epochs=2\n",
            "Validation Accuracy: 0.1123\n",
            "Testing: lr=0.1, hidden_size=256, batch_size=64, epochs=2\n",
            "Validation Accuracy: 0.3708\n",
            "Testing: lr=0.1, hidden_size=256, batch_size=128, epochs=2\n",
            "Validation Accuracy: 0.2901\n",
            "\n",
            "Best Hyperparameters:\n",
            "{'learning_rate': 0.001, 'hidden_size': 256, 'batch_size': 32, 'num_epochs': 2}\n",
            "Best Validation Accuracy: 0.9736\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "class ComplexMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "        self.bn6 = nn.BatchNorm1d(128)\n",
        "        self.bn7 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn6(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn7(self.fc4(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "train_size = int(0.8 * len(train_set))\n",
        "val_size = len(train_set) - train_size\n",
        "train_set, val_set = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "hidden_sizes = [64, 128, 256]\n",
        "batch_sizes = [32, 64, 128]\n",
        "num_epochs_list = 2\n",
        "\n",
        "best_hyperparams = {}\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_size in hidden_sizes:\n",
        "        for batch_size in batch_sizes:\n",
        "              print(f\"Testing: lr={lr}, hidden_size={hidden_size}, batch_size={batch_size}, epochs={2}\")\n",
        "\n",
        "              train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "              val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "              model = MLP(hidden_size)\n",
        "              criterion = nn.CrossEntropyLoss()\n",
        "              optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "              val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs_list)\n",
        "\n",
        "              if val_acc > best_accuracy:\n",
        "                  best_accuracy = val_acc\n",
        "                  best_hyperparams = {\n",
        "                      \"learning_rate\": lr,\n",
        "                      \"hidden_size\": hidden_size,\n",
        "                      \"batch_size\": batch_size,\n",
        "                      \"num_epochs\": num_epochs_list,\n",
        "                  }\n",
        "\n",
        "              print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(best_hyperparams)\n",
        "print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ5OmUPOUsyn"
      },
      "source": [
        "## Задание 2.\n",
        "\n",
        "1. Из встроенных датасетов torchvision загрузите тестовую и обучающую выборки из EMNIST часть (split) Balanced или Letters.\n",
        "2. Постройте код модели и ее обучения, используйте разные варианты скорости обучения, количества параметров в скрытых слоя модели, количества эпох, размера батчей.\n",
        "2. Выведите гиперпараметры лучшей модели.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nghA7nHAVILV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuOiOi11VIur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474aeaf0-b86c-4451-f46c-24aaf5e1baaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with LR=0.001, Hidden units=64, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 1.3050\n",
            "Epoch [2/3], Loss: 0.8720\n",
            "Epoch [3/3], Loss: 0.7690\n",
            "Test Accuracy: 75.96%\n",
            "\n",
            "Training with LR=0.001, Hidden units=64, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 1.3834\n",
            "Epoch [2/3], Loss: 0.9031\n",
            "Epoch [3/3], Loss: 0.7782\n",
            "Test Accuracy: 76.02%\n",
            "\n",
            "Training with LR=0.001, Hidden units=64, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 1.5103\n",
            "Epoch [2/3], Loss: 0.9854\n",
            "Epoch [3/3], Loss: 0.8409\n",
            "Test Accuracy: 74.45%\n",
            "\n",
            "Training with LR=0.001, Hidden units=128, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 1.0860\n",
            "Epoch [2/3], Loss: 0.6853\n",
            "Epoch [3/3], Loss: 0.6049\n",
            "Test Accuracy: 79.91%\n",
            "\n",
            "Training with LR=0.001, Hidden units=128, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 1.1356\n",
            "Epoch [2/3], Loss: 0.6875\n",
            "Epoch [3/3], Loss: 0.5905\n",
            "Test Accuracy: 80.34%\n",
            "\n",
            "Training with LR=0.001, Hidden units=128, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 1.3082\n",
            "Epoch [2/3], Loss: 0.7691\n",
            "Epoch [3/3], Loss: 0.6428\n",
            "Test Accuracy: 79.56%\n",
            "\n",
            "Training with LR=0.001, Hidden units=256, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 0.9585\n",
            "Epoch [2/3], Loss: 0.6027\n",
            "Epoch [3/3], Loss: 0.5346\n",
            "Test Accuracy: 81.54%\n",
            "\n",
            "Training with LR=0.001, Hidden units=256, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 1.0173\n",
            "Epoch [2/3], Loss: 0.6013\n",
            "Epoch [3/3], Loss: 0.5177\n",
            "Test Accuracy: 81.93%\n",
            "\n",
            "Training with LR=0.001, Hidden units=256, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 1.1101\n",
            "Epoch [2/3], Loss: 0.6261\n",
            "Epoch [3/3], Loss: 0.5252\n",
            "Test Accuracy: 81.49%\n",
            "\n",
            "Training with LR=0.01, Hidden units=64, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 1.7900\n",
            "Epoch [2/3], Loss: 1.5979\n",
            "Epoch [3/3], Loss: 1.5724\n",
            "Test Accuracy: 54.78%\n",
            "\n",
            "Training with LR=0.01, Hidden units=64, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 1.5259\n",
            "Epoch [2/3], Loss: 1.3045\n",
            "Epoch [3/3], Loss: 1.2652\n",
            "Test Accuracy: 62.37%\n",
            "\n",
            "Training with LR=0.01, Hidden units=64, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 1.3574\n",
            "Epoch [2/3], Loss: 1.0865\n",
            "Epoch [3/3], Loss: 1.0500\n",
            "Test Accuracy: 67.82%\n",
            "\n",
            "Training with LR=0.01, Hidden units=128, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 1.8112\n",
            "Epoch [2/3], Loss: 1.6531\n",
            "Epoch [3/3], Loss: 1.6221\n",
            "Test Accuracy: 53.29%\n",
            "\n",
            "Training with LR=0.01, Hidden units=128, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 1.5260\n",
            "Epoch [2/3], Loss: 1.3639\n",
            "Epoch [3/3], Loss: 1.3299\n",
            "Test Accuracy: 60.95%\n",
            "\n",
            "Training with LR=0.01, Hidden units=128, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 1.3557\n",
            "Epoch [2/3], Loss: 1.1003\n",
            "Epoch [3/3], Loss: 1.0783\n",
            "Test Accuracy: 67.27%\n",
            "\n",
            "Training with LR=0.01, Hidden units=256, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 1.9250\n",
            "Epoch [2/3], Loss: 1.6957\n",
            "Epoch [3/3], Loss: 1.6565\n",
            "Test Accuracy: 52.12%\n",
            "\n",
            "Training with LR=0.01, Hidden units=256, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 1.5206\n",
            "Epoch [2/3], Loss: 1.3145\n",
            "Epoch [3/3], Loss: 1.2823\n",
            "Test Accuracy: 64.51%\n",
            "\n",
            "Training with LR=0.01, Hidden units=256, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 1.3635\n",
            "Epoch [2/3], Loss: 1.0703\n",
            "Epoch [3/3], Loss: 1.0403\n",
            "Test Accuracy: 68.79%\n",
            "\n",
            "Training with LR=0.1, Hidden units=64, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 3.9464\n",
            "Epoch [2/3], Loss: 3.8801\n",
            "Epoch [3/3], Loss: 3.8802\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Training with LR=0.1, Hidden units=64, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 4.0177\n",
            "Epoch [2/3], Loss: 3.8698\n",
            "Epoch [3/3], Loss: 3.8709\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Training with LR=0.1, Hidden units=64, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 3.8310\n",
            "Epoch [2/3], Loss: 3.4583\n",
            "Epoch [3/3], Loss: 3.4369\n",
            "Test Accuracy: 6.60%\n",
            "\n",
            "Training with LR=0.1, Hidden units=128, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 4.1022\n",
            "Epoch [2/3], Loss: 3.8799\n",
            "Epoch [3/3], Loss: 3.8802\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Training with LR=0.1, Hidden units=128, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 4.6165\n",
            "Epoch [2/3], Loss: 3.8708\n",
            "Epoch [3/3], Loss: 3.8700\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Training with LR=0.1, Hidden units=128, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 4.7786\n",
            "Epoch [2/3], Loss: 3.8643\n",
            "Epoch [3/3], Loss: 3.8645\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Training with LR=0.1, Hidden units=256, Batch size=32, Epochs=3\n",
            "Epoch [1/3], Loss: 4.6009\n",
            "Epoch [2/3], Loss: 3.8793\n",
            "Epoch [3/3], Loss: 3.8797\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Training with LR=0.1, Hidden units=256, Batch size=64, Epochs=3\n",
            "Epoch [1/3], Loss: 5.4169\n",
            "Epoch [2/3], Loss: 3.8714\n",
            "Epoch [3/3], Loss: 3.8705\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Training with LR=0.1, Hidden units=256, Batch size=128, Epochs=3\n",
            "Epoch [1/3], Loss: 6.8728\n",
            "Epoch [2/3], Loss: 3.8649\n",
            "Epoch [3/3], Loss: 3.8643\n",
            "Test Accuracy: 2.13%\n",
            "\n",
            "Best model parameters: {'learning_rate': 0.001, 'hidden_units': 256, 'batch_size': 64, 'epochs': 3}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "split_type = 'balanced'\n",
        "num_classes = 47 if split_type == 'balanced' else 26\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.EMNIST(root='./data', split=split_type, train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.EMNIST(root='./data', split=split_type, train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "def get_dataloaders(batch_size):\n",
        "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "    return trainloader, testloader\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, hidden_units=128, num_classes=47):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_units)\n",
        "        self.fc2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.fc3 = nn.Linear(hidden_units, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, trainloader, num_epochs=3):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(trainloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def evaluate_model(model, testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "hidden_units_list = [64, 128, 256]\n",
        "batch_sizes = [32, 64, 128]\n",
        "num_epochs = 3\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for hidden_units in hidden_units_list:\n",
        "        for batch_size in batch_sizes:\n",
        "            print(f\"\\nTraining with LR={lr}, Hidden units={hidden_units}, Batch size={batch_size}, Epochs={num_epochs}\")\n",
        "\n",
        "\n",
        "            model = Net(hidden_units=hidden_units, num_classes=num_classes)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            trainloader, testloader = get_dataloaders(batch_size)\n",
        "\n",
        "            train_model(model, criterion, optimizer, trainloader, num_epochs)\n",
        "\n",
        "            accuracy = evaluate_model(model, testloader)\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = {\n",
        "                    'learning_rate': lr,\n",
        "                    'hidden_units': hidden_units,\n",
        "                    'batch_size': batch_size,\n",
        "                    'epochs': num_epochs\n",
        "                }\n",
        "\n",
        "print(\"\\nBest model parameters:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YqlOsRgONcT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}